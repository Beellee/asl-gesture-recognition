# ü§ü ASL Gesture Recognition (American Sign Language)

Este proyecto ofrece un pipeline completo para el reconocimiento de gestos de Lengua de Se√±as Americana (ASL) basado en keypoints extra√≠dos con MediaPipe. Incluye desde la captura de v√≠deos y extracci√≥n de keypoints, hasta la augmentaci√≥n de datos, el preprocesamiento, el entrenamiento de modelos (LSTM y TCN) con caracter√≠sticas auxiliares (duraci√≥n y fracci√≥n de manos activas), la evaluaci√≥n de desempe√±o y el despliegue de una API para inferencia en tiempo real.

---

## Estructura General

* **app/**
  * `app.py`: expone un endpoint REST que recibe un video, extrae keypoints de manos con MediaPipe, aplica el LSTM entrenado para clasificar el gesto y devuelve el signo predicho con su confianza
  * `index.html` / `index.css` /`index.js`: frontend de la app
  * `requirements.txt`: lista de dependencias necesarias (PyTorch, FastAPI, MediaPipe, OpenCV, scikit-learn, NumPy, etc.).

* **code/**
  * `record_videos.html`: app utilizada para grabar datos de entrenamiento 
  * `train_model.ipynb`: flujo completo utilizado para entrenar LSTM y TCN.contiene: 
      - extracci√≥n de keypoints con MediaPipe
      - aumento de datos / ruido
      - metricas para cuantificar el ruido
      - detecci√≥n de la "ventana activa"
      - calculo de m√©tricas auxiliares (duracion original / proporcion de mano derecha/izquierda utilizada en el signo)
      - flujos de procesamiento de los datos para alimentarlos a cada red neuronal
      - entrenamiento y m√©tricas
  * `documentation`: scripts para mostrar las partes mas visuales del proyecto en la memoria.

* **data/** 
  * `keypoints/`: v√≠deos crudos organizados por signo.
  * `keypoints_augmented/`: keypoints obtenidos tras extracci√≥n y augmentaci√≥n, organizados por signo.
  * `keypoints_active_window/`: secuencias recortadas seg√∫n la ventana de movimiento activo para cada clip (sin forzar longitud).
  * `keypoints_fixed/`: secuencias normalizadas y ajustadas a longitud fija (por defecto 73 frames), listas para entrenar la LSTM.
  * `keypoints_metadata/`: archivos JSON que describen metadatos de cada secuencia (duraci√≥n original, √≠ndices de inicio/fin de la ventana activa, longitud objetivo).

  (por motivos de privacidad esta carpeta no se comparte)

* **shared\_elements/**
  * `lstm.py`: definici√≥n de la clase `SignLSTM`, que consume secuencias de keypoints, duraci√≥n y fracciones de manos para generar logits de clase.
  * `tcn.py`: definici√≥n de la clase `SignTCN`, con bloques de convoluciones causales dilatadas, pooling global y fusi√≥n de caracter√≠sticas auxiliares.
  * `best_model.pt`: mejor modelo de LSTM encontrado durante el entrenamiento.
  * `best_model_tcn.pt`: mejor modelo de TCN encontrado durante el entrenamiento.
  * `shared_utils.py`: utilidades compartidas (instanciaci√≥n de MediaPipe, extracci√≥n de keypoints, detecci√≥n de ventana activa, ajuste de longitud, c√°lculo de fracciones de manos activas).
  * `X_mean.npy` / `X_std.npy`: vectores de media y desviaci√≥n est√°ndar usados para normalizar las caracter√≠sticas de cada frame.
  * `label_encoder.pkl`: objeto de codificaci√≥n de etiquetas (mapa entre √≠ndice y nombre de signo).



---

## Descripci√≥n del Pipeline

1. **Extracci√≥n de Keypoints**

   * Se capturan v√≠deos de cada signo con c√°mara frontal.
   * Se procesan mediante MediaPipe Holistic para obtener √∫nicamente los keypoints de manos (cada frame resulta en un array de 42 puntos con coordenadas (x, y, z)).
   * Se guardan como archivos `.npy` organizados en carpetas por signo.

2. **Aumento de Datos**

   * A cada secuencia original de keypoints se le aplican transformaciones aleatorias:

     * **Jitter Gaussiano**: se a√±ade ruido a cada coordenada.
     * **Escalado Global**: se multiplica por un factor aleatorio cercano a 1.
     * **Traslaci√≥n Global**: se desplaza toda la nube de puntos un peque√±o porcentaje del rango.
     * **Rotaci√≥n en Plano XY**: giro alrededor del eje Z.
     * **Dropout de Keypoints**: se fijan aleatoriamente algunos puntos a cero (simula oclusiones).
     * **Warp Temporal**: duplicaci√≥n o eliminaci√≥n espor√°dica de frames para simular cambios de velocidad.
   * De cada muestra original se generan varias versiones augmentadas para enriquecer la diversidad del dataset.

3. **Detecci√≥n de Ventana Activa**

   * Para cada secuencia augmentada, se calcula la magnitud de movimiento frame a frame, sumando las distancias L2 entre keypoints consecutivos.
   * Se define un umbral proporcional al m√°ximo de velocidad, y se identifican los primeros y √∫ltimos frames donde el movimiento supera ese umbral.
   * La regi√≥n entre esos √≠ndices define la ‚Äúventana activa‚Äù del gesto (evita incluir muchos frames sin movimiento).

4. **Ajuste de Longitud Fija**

   * La ventana activa detectada se recorta o paddea para tener siempre una longitud constante (por ejemplo, 73 frames).
   * Si la duraci√≥n excede la longitud deseada, se realiza un center-crop. Si dura menos, se a√±aden frames nulos a ambos lados de manera equitativa.
   * De este modo, todas las secuencias para la LSTM comparten la misma forma y pueden cargarse en batch sin necesidad de padding adicional en el modelado.

5. **C√°lculo de Caracter√≠sticas Auxiliares**

   * La **duraci√≥n normalizada** se obtiene dividiendo el n√∫mero de frames de la ventana activa por la longitud fija (e.g., duraci√≥n/73).
   * La **fracci√≥n de manos activas** se calcula separando velocidades left/right, definiendo un umbral de movimiento y midiendo el porcentaje de frames √∫tiles en los que cada mano aparece y realmente se mueve.
   * Estos valores aportan informaci√≥n global sobre el gesto (ej. un gesto largo vs. corto, uso preferente de mano izquierda vs. derecha).

6. **Normalizaci√≥n de Caracter√≠sticas**

   * Se recopila todo el conjunto de entrenamiento, se concatenan las secuencias `(num_muestras, 73, 126)` (42 keypoints √ó 3 coordenadas = 126 features) y se calcula la media y desviaci√≥n est√°ndar por feature.
   * Cada secuencia se normaliza restando la media y dividiendo por la desviaci√≥n est√°ndar, para estabilizar el entrenamiento de la red y acelerar la convergencia.

---

## Modelos

### 1. LSTM con Caracter√≠sticas Auxiliares

* **Entrada**: Tensores de forma `(B, 73, 126)` m√°s vectores auxiliares `(B,1)` para duraci√≥n y `(B,2)` para fracci√≥n de uso de cada mano.

* **Arquitectura**:

  1. Capa LSTM (con 2 capas ocultas y tama√±o de estado oculto configurable).
  2. Se extrae el √∫ltimo estado oculto (representaci√≥n fija del gesto).
  3. Se aplica Layer Normalization sobre ese vector.
  4. Se concatenan las caracter√≠sticas auxiliares (3 valores adicionales).
  5. Dos capas fully-connected: primero con 64 unidades y ReLU, luego capa de salida con tantas neuronas como clases (15).
  6. Softmax final impl√≠cito durante la inferencia para convertir logits en probabilidades.

* **Objetivo**: Minimizar entrop√≠a cruzada con etiquetas codificadas.

* **Regularizaci√≥n**: Dropout interno en las capas LSTM (si corresponde), y Early-Stopping basado en la p√©rdida de validaci√≥n con paciencia configurable.

* **Artefactos Generados**:

  * Pesos del mejor modelo (.pt)
  * LabelEncoder para traducir √≠ndices de clase a nombres de signo
  * Vectores `X_mean.npy` y `X_std.npy` para normalizar nuevas muestras en producci√≥n

### 2. TCN (Temporal Convolutional Network) con Caracter√≠sticas Auxiliares

* **Entrada**: Tensores de forma `(B, T, 126)` donde `T` puede variar entre muestras (no es necesario recortar a longitud fija).

* **Arquitectura**:

  1. Capa inicial 1√ó1 que adapta el n√∫mero de features (126) a un n√∫mero deseado de ‚Äúcanales‚Äù (p. ej. 64).
  2. Serie de **Bloques Residuales Dilatados**: cada uno realiza:

     * Convoluci√≥n 1D causal con dilataci√≥n predeterminada (se omite ‚Äúfuturo‚Äù para mantener causalidad).
     * Recorte de los excesos de padding para preservar la longitud original de la secuencia.
     * ReLU, Dropout, y suma residual (a√±ade la entrada proyectada al resultado).
  3. Tras aplicar todos los bloques, se emplea un **Pooling Global Adaptativo** que colapsa la dimensi√≥n temporal variable a longitud 1 (elige el valor m√°ximo a lo largo del tiempo para cada canal).
  4. Se concatenan las caracter√≠sticas auxiliares (duraci√≥n normalizada y fracciones de manos, total 3 valores).
  5. Dos capas fully-connected: primero con 64 unidades y ReLU, luego capa de salida con n√∫mero de clases.

* **Ventaja Principal**:

  * Puede procesar secuencias de longitud variable sin necesidad de recorte/padding previo.
  * Las convoluciones dilatadas permiten abarcar contextos temporales m√°s largos (mirar varios pasos atr√°s) sin incrementar excesivamente la profundidad.
  * El uso de conexiones residuales facilita el flujo de gradiente y la estabilizaci√≥n de redes profundas.

* **Entrenamiento y Evaluaci√≥n**:

  * Se usan los mismos conjuntos de entrenamiento y validaci√≥n que para la LSTM, salvo que las secuencias no tienen longitud fija (TCN aplica pooling global para unificar la dimensi√≥n temporal).
  * Tambi√©n se incluyen Early-Stopping y c√°lculo de m√©tricas de clasificaci√≥n (classification report y matriz de confusi√≥n).

---

## Evaluaci√≥n y M√©tricas

Durante y tras el entrenamiento se eval√∫a el desempe√±o usando:

* **Accuracy (Exactitud)**
* **Classification Report**: Precision, recall y F1-score por clase.
* **Matriz de Confusi√≥n**: Visualizaci√≥n de aciertos y errores por par (clase verdadera, clase predicha).
* **M√©tricas de Separabilidad** (durante la fase de augmentaci√≥n):

  * **MSE / RMSE** entre secuencia original y augmentada para cuantificar desviaciones promedio de keypoints.
  * **DTW (Dynamic Time Warping)** para medir distancia entre secuencias con posible desalineaci√≥n temporal (warp).
  * **SNR (Signal-to-Noise Ratio)** para calibrar nivel de ruido a√±adido.
  * **Distancia Intra/Inter-Clase**: comparar promedios de distancias (MSE o DTW) dentro de la misma clase versus entre clases, asegurando que intra-clase sea mucho menor que inter-clase.
  * **Silhouette Score** y **Davies‚ÄìBouldin Index** para evaluar qu√© tan bien separadas quedan las clases en el espacio de caracter√≠sticas tras augmentaci√≥n.

---

## Despliegue con FastAPI

Para ofrecer una demostraci√≥n en tiempo real, se ha implementado un servidor FastAPI que:

1. Recibe un v√≠deo desde el navegador (JavaScript en front-end).
2. Guarda temporalmente el archivo en disco.
3. Extrae los keypoints usando MediaPipe Holistic.
4. Detecta la ventana activa, ajusta la longitud a 73 frames y normaliza.
5. Calcula duraci√≥n normalizada y fracciones de manos activas.
6. Construye tensores y realiza inferencia con el modelo LSTM entrenado (o TCN si se decide reemplazar).
7. Devuelve la predicci√≥n de signo y un puntaje de confianza (probabilidad de la clase m√°xima).

La comunicaci√≥n con el front-end se realiza mediante JSON, por ejemplo:

```json
{ "sign": "hello", "confidence": 0.92 }
```

Lo cual permite mostrar la predicci√≥n al usuario en una simple aplicaci√≥n web.

Para levantar la app ejecutar desde la terminal 
```
python3 -m uvicorn app:app --reload    # backend
python3 -m http.server 3000            # frontend
```

---

## Ejecuci√≥n Paso a Paso

1. **Instalar Dependencias**

   * Asegurarse de contar con Python 3.8+ y los paquetes indicados en `requirements.txt`.

2. **Extraer Keypoints**

   * Ejecutar rutina de extracci√≥n para convertir v√≠deos crudos en archivos `.npy` de keypoints.

3. **Aumentar el Dataset**

   * Aplicar augmentaci√≥n con los par√°metros deseados, generando nuevas muestras.

4. **Preprocesar y Ajustar Longitud**

   * Detectar ventana activa y pad/crop a longitud fija para preparar datos de LSTM. Para TCN, basta con recortar la ventana activa sin forzar longitud.

5. **Entrenar Modelos**

   * En los notebooks (`train_lstm.ipynb` y `train_tcn.ipynb`), seguir el flujo:

     1. Cargar datos con DataLoader (pasan secuencias, duraciones y fracciones de manos).
     2. Normalizar caracter√≠sticas con vectores `X_mean.npy` y `X_std.npy`.
     3. Inicializar y ajustar LSTM o TCN con Early-Stopping.
     4. Guardar mejores pesos, `label_encoder.pkl`, `X_mean.npy` y `X_std.npy`.

6. **Evaluar Desempe√±o**

   * Generar el informe de clasificaci√≥n y matriz de confusi√≥n para cuantificar precision, recall, F1-score y exactitud.

7. **Desplegar API**

   * Desde la carpeta `app/`, ejecutar el servidor con Uvicorn; la ruta `/predict` queda disponible para recibir v√≠deos y devolver predicciones en JSON.

---

## Ventajas y Limitaciones

* **Ventajas**

  * Procesamiento s√≥lo de keypoints de manos (42 puntos √ó 3 coordenadas) (mas ligero computacionalmente).
  * Aumento de datos robusto que emula ruidos, variaciones de c√°mara y velocidad de gesto.
  * Dos arquitecturas de modelado: LSTM  y TCN.
  * Incorporaci√≥n de caracter√≠sticas globales (duraci√≥n y uso de manos) que aumenta la capacidad de separaci√≥n.
  * Pipeline modular y reutilizable, con notebooks bien documentados y API lista para producci√≥n.

* **Limitaciones y Futuras Mejoras**

  * Solo se consideran keypoints de manos; no se aprovecha la informaci√≥n del cuerpo o la cara.
  * Actualmente el LSTM requiere longitud fija (pad/crop), lo cual puede introducir informaci√≥n no realista (frames nulos).
  * El TCN maneja longitud variable, pero puede requerir ajuste de hiperpar√°metros de dilataci√≥n seg√∫n dataset.
  * El modelo podr√≠a beneficiarse de arquitecturas que exploten la topolog√≠a de la mano (e.g., GCNs para grafos esquel√©ticos).
  * Se podr√≠a incluir un front-end m√°s completo (UI) y m√©tricas en tiempo real para mejorar la experiencia final.
  * Se podr√≠a mejorar la forma de detectar la ventana activa y el porcentaje de uso de manos para evitar falsos positivos.


