# ðŸ¤Ÿ ASL Gesture Recognition (American Sign Language)

This project enables users to record sign language gestures using a webcam, process them using MediaPipe, and classify the sign in real-time using a trained LSTM model. Built with **FastAPI**, **PyTorch**, and **JavaScript**, it combines deep learning, computer vision, and web technologies in a full-stack ASL recognition system.

## Project Structure
asl-gesture-recognition/ 
â”œâ”€â”€ app/ # FastAPI backend 
â”‚ â”œâ”€â”€ app.py # Prediction endpoint using PyTorch 
â”‚ â”œâ”€â”€ index.html
â”‚ â”œâ”€â”€ index.css
â”‚ â”œâ”€â”€ index.js
â”‚ â”œâ”€â”€ model.pth # Trained LSTM model 
â”‚ â”œâ”€â”€ label_encoder.pkl # Encoded class labels 
â”‚ â””â”€â”€ scaler.pkl # Mean/std for normalization 
â”‚
â”œâ”€â”€ code/ 
â”‚ â”œâ”€â”€ hand_landmarker.task
â”‚ â””â”€â”€ train_model.ipynb
â”‚
â””â”€â”€ data/ 
  â”œâ”€â”€ keypoints
  â””â”€â”€ videos


---

# Train Your Own Model 
Use train_model.py to:
 - Load .npy keypoints
 - Normalize data
 - Train an LSTM model
 - Save model.pth, label_encoder.pkl, and scaler.pkl

Input Format:
Each .npy file should contain a sequence with shape (T, 75, 3) â€” 75 keypoints from pose and hands, each with (x, y, z).

## Use 

```bash
# Create a virtual environment (optional but recommended)
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install fastapi uvicorn torch mediapipe scikit-learn python-multipart opencv-python

# RUN THE APP 
cd app
# backend
python3 -m uvicorn app:app --reload
# frontend
python3 -m http.server 3000
