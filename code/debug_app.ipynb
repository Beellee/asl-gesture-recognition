{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00ffa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRIMERA PARTE \n",
    "# Load keypoint sequence\n",
    "seq = np.load(\"../app/debug_sequence.npy\")\n",
    "\n",
    "# Load scaler and normalize\n",
    "with open(\"../app/scaler.pkl\", \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n",
    "seq = (seq - scaler['mean']) / scaler['std']\n",
    "tensor = torch.from_numpy(seq.astype(np.float32)).unsqueeze(0)\n",
    "\n",
    "# Load model\n",
    "#model = SignLSTM(input_size=126, hidden_size=128, num_layers=2, num_classes=15)\n",
    "#model.load_state_dict(torch.load(\"../app/model.pth\", map_location=\"cpu\"))\n",
    "model.load_state_dict(torch.load(\"best_model.pt\", map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    logits = model(tensor)\n",
    "    probs = torch.softmax(logits, dim=1).numpy()[0]\n",
    "    predicted_idx = int(probs.argmax())\n",
    "\n",
    "# Load label encoder\n",
    "with open(\"../app/label_encoder.pkl\", \"rb\") as f:\n",
    "    le = pickle.load(f)\n",
    "\n",
    "print(\"🔍 Prediction:\", le.inverse_transform([predicted_idx])[0])\n",
    "print(\"Probabilities:\")\n",
    "for i, p in enumerate(probs):\n",
    "    print(f\"  {le.inverse_transform([i])[0]}: {p:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac137f2",
   "metadata": {},
   "source": [
    "#### output app \n",
    "```\n",
    "✅ Keypoint sequence saved to debug_sequence.npy\n",
    "Raw logits: tensor([[-0.9475,  0.4579,  1.5646, -0.6141,  7.0239,  0.4305, -1.7777, -2.6337,\n",
    "         -2.9380,  0.2447, -1.1950, -1.7126,  1.9589, -0.2472, -0.5548]])\n",
    "Probabilities:\n",
    "  bad: 0.0003\n",
    "  bye: 0.0014\n",
    "  good: 0.0042\n",
    "  hello: 0.0005\n",
    "  help: 0.9831\n",
    "  love: 0.0013\n",
    "  me: 0.0001\n",
    "  no: 0.0001\n",
    "  please: 0.0000\n",
    "  sorry: 0.0011\n",
    "  stop: 0.0003\n",
    "  thank_you: 0.0002\n",
    "  world: 0.0062\n",
    "  yes: 0.0007\n",
    "  you: 0.0005\n",
    "Prediction: help | Confidence: 0.9830814003944397\n",
    "```\n",
    "\n",
    "#### output sequencia app en NB\n",
    "\n",
    "```\n",
    "🔍 Prediction: love\n",
    "Probabilities:\n",
    "  bad: 0.0082\n",
    "  bye: 0.0220\n",
    "  good: 0.0231\n",
    "  hello: 0.2244\n",
    "  help: 0.0107\n",
    "  love: 0.6110\n",
    "  me: 0.0049\n",
    "  no: 0.0004\n",
    "  please: 0.0449\n",
    "  sorry: 0.0044\n",
    "  stop: 0.0030\n",
    "  thank_you: 0.0017\n",
    "  world: 0.0022\n",
    "  yes: 0.0386\n",
    "  you: 0.0006\n",
    "```\n",
    "\n",
    "la misma secuencia .npy genera dos predicciones diferentes en la app y en el notebook. Eso confirma que no es un problema de datos ni del modelo entrenado en sí, sino de inconsistencias entre los entornos o la forma en que se cargan los modelos y artefactos.\n",
    "(La prediccion correcta es Love)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ade1af06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Prediction from debug_sequence2.npy: good\n",
      "Probabilities:\n",
      "  bad: 0.0020\n",
      "  bye: 0.0002\n",
      "  good: 0.9620\n",
      "  hello: 0.0001\n",
      "  help: 0.0053\n",
      "  love: 0.0006\n",
      "  me: 0.0190\n",
      "  no: 0.0005\n",
      "  please: 0.0007\n",
      "  sorry: 0.0015\n",
      "  stop: 0.0057\n",
      "  thank_you: 0.0006\n",
      "  world: 0.0009\n",
      "  yes: 0.0001\n",
      "  you: 0.0009\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sys\n",
    "sys.path.append('../models')\n",
    "from lstm import SignLSTM\n",
    "\n",
    "# ─── LOAD SEQUENCE ────────────────────────────────────────────────\n",
    "seq = np.load(\"../app/debug_sequence2.npy\") \n",
    "#seq = np.load(\"../data/keypoints_augmented/good/good_1_aug0.npy\")\n",
    "\n",
    "# ─── LOAD SCALER ──────────────────────────────────────────────────\n",
    "with open(\"../app/scaler.pkl\", \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n",
    "seq = (seq - scaler['mean']) / scaler['std']\n",
    "tensor = torch.from_numpy(seq.astype(np.float32)).unsqueeze(0)\n",
    "\n",
    "# ─── LOAD MODEL ───────────────────────────────────────────────────\n",
    "# Debes pasar los parámetros correctos\n",
    "model = SignLSTM(input_size=126, hidden_size=128, num_layers=2, num_classes=15)\n",
    "model.load_state_dict(torch.load(\"best_model.pt\", map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "# ─── INFERENCE ────────────────────────────────────────────────────\n",
    "with torch.no_grad():\n",
    "    logits = model(tensor)\n",
    "    probs = torch.softmax(logits, dim=1).numpy()[0]\n",
    "    predicted_idx = int(probs.argmax())\n",
    "\n",
    "# ─── LABELS ───────────────────────────────────────────────────────\n",
    "with open(\"../app/label_encoder.pkl\", \"rb\") as f:\n",
    "    le = pickle.load(f)\n",
    "\n",
    "# ─── OUTPUT ───────────────────────────────────────────────────────\n",
    "print(\"🔍 Prediction from debug_sequence2.npy:\", le.inverse_transform([predicted_idx])[0])\n",
    "print(\"Probabilities:\")\n",
    "for i, p in enumerate(probs):\n",
    "    print(f\"  {le.inverse_transform([i])[0]}: {p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2394fe2e",
   "metadata": {},
   "source": [
    "````\n",
    "✅ Keypoint sequence saved to debug_sequence2.npy\n",
    "Raw logits: tensor([[-1.8610,  1.0745,  0.4421, -1.6416, -0.4929, -3.4781, -2.0141,  8.7493,\n",
    "         -2.5912, -2.1223,  0.0804,  1.8194, -2.7973,  1.6758, -0.6637]])\n",
    "Probabilities:\n",
    "  bad: 0.0000\n",
    "  bye: 0.0005\n",
    "  good: 0.0002\n",
    "  hello: 0.0000\n",
    "  help: 0.0001\n",
    "  love: 0.0000\n",
    "  me: 0.0000\n",
    "  no: 0.9970\n",
    "  please: 0.0000\n",
    "  sorry: 0.0000\n",
    "  stop: 0.0002\n",
    "  thank_you: 0.0010\n",
    "  world: 0.0000\n",
    "  yes: 0.0008\n",
    "  you: 0.0001\n",
    "Prediction: no | Confidence: 0.9970000386238098\n",
    "\n",
    "````\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0f5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to visualize keypoints from a .npy file\n",
    "def visualize_keypoints_hands(npy_file):\n",
    "    keypoints = np.load(npy_file)  # Load keypoints data\n",
    "    # Desaplanar si viene en forma (T, 126)\n",
    "    if keypoints.ndim == 2 and keypoints.shape[1] == 126:\n",
    "        keypoints = keypoints.reshape((-1, 42, 3))\n",
    "        \n",
    "    num_frames = keypoints.shape[0]\n",
    "\n",
    "    print(f\"Visualizing {os.path.basename(npy_file)} ({num_frames} frames)\")\n",
    "\n",
    "    # Create a blank image for visualization\n",
    "    img_size = 500\n",
    "    blank_frame = np.ones((img_size, img_size, 3), dtype=np.uint8) * 255\n",
    "\n",
    "    # Define landmarks (POSE = 33, HAND = 21 per hand)\n",
    "    POSE_LANDMARKS = 33\n",
    "    HAND_LANDMARKS = 21\n",
    "    TOTAL_KEYPOINTS = 2 * HAND_LANDMARKS\n",
    "\n",
    "\n",
    "    # Animate the keypoints frame by frame\n",
    "    for frame_idx in range(num_frames):\n",
    "        img = blank_frame.copy()\n",
    "        frame_keypoints = keypoints[frame_idx]\n",
    "\n",
    "        # Normalize & scale keypoints to fit the image\n",
    "        scaled_keypoints = (frame_keypoints[:, :2] * img_size).astype(int)\n",
    "\n",
    "        # Draw left hand landmarks\n",
    "        for i in range(HAND_LANDMARKS):\n",
    "            x, y = scaled_keypoints[i]\n",
    "            cv2.circle(img, (x, y), 3, (255, 0, 0), -1)  # Blue for left hand\n",
    "\n",
    "        # Draw right hand landmarks\n",
    "        for i in range(HAND_LANDMARKS):\n",
    "            x, y = scaled_keypoints[HAND_LANDMARKS + i]\n",
    "            cv2.circle(img, (x, y), 3, (0, 255, 0), -1)  # Green for right hand\n",
    "\n",
    "        # Show the frame\n",
    "        cv2.imshow(\"Keypoints Visualization\", img)\n",
    "        if cv2.waitKey(50) & 0xFF == ord('q'):  # Press 'q' to quit\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\" Visualization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf8c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing debug_sequence3.npy (60 frames)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 18:34:54.859 Python[34464:1584305] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-05-17 18:34:54.859 Python[34464:1584305] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Visualization complete.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Folder where keypoint files are stored\n",
    "debug = \"../app/debug_sequence3.npy\"\n",
    "# Visualize the first keypoint file (change index for other files)\n",
    "visualize_keypoints_hands(debug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de78487",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#forzar salida de la visualizacion\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3aa2e835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 keypoint files.\n",
      "\n",
      "🔍 Prediction for file: good_31_aug0.npy\n",
      "👉 Predicted sign: hello\n",
      "📊 Probabilities:\n",
      "  bad: 0.0005\n",
      "  bye: 0.0093\n",
      "  good: 0.0010\n",
      "  hello: 0.6619\n",
      "  help: 0.0010\n",
      "  love: 0.0016\n",
      "  me: 0.0004\n",
      "  no: 0.0032\n",
      "  please: 0.0107\n",
      "  sorry: 0.0020\n",
      "  stop: 0.0019\n",
      "  thank_you: 0.2691\n",
      "  world: 0.0045\n",
      "  yes: 0.0322\n",
      "  you: 0.0009\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# ─── IMPORTAR CLASE DEL MODELO ────────────────────────────────────────────────\n",
    "sys.path.append('../models')\n",
    "from lstm import SignLSTM\n",
    "\n",
    "# ─── CONFIGURACIÓN ────────────────────────────────────────────────────────────\n",
    "keypoints_folder = \"../data/tracking_keypoints/keypoints_augmented/good\"\n",
    "SEQUENCE_LENGTH = 60\n",
    "model_path = \"../code/best_model.pt\"\n",
    "scaler_path = \"../app/scaler.pkl\"\n",
    "label_encoder_path = \"../app/label_encoder.pkl\"\n",
    "\n",
    "# ─── CARGAR SECUENCIA DE UN .NPY DEL DATASET ──────────────────────────────────\n",
    "npy_files = [f for f in os.listdir(keypoints_folder) if f.endswith(\".npy\")]\n",
    "print(f\"Found {len(npy_files)} keypoint files.\")\n",
    "\n",
    "# Seleccionar uno (por ejemplo el número 100)\n",
    "file_path = os.path.join(keypoints_folder, npy_files[100])\n",
    "seq = np.load(file_path)\n",
    "\n",
    "# Validación de forma esperada\n",
    "if seq.shape[1:] != (42, 3):\n",
    "    raise ValueError(f\"❌ Unexpected shape in {file_path}: {seq.shape}\")\n",
    "\n",
    "# Recortar o rellenar a 30 frames\n",
    "\n",
    "if seq.shape[0] < SEQUENCE_LENGTH:\n",
    "    pad_width = SEQUENCE_LENGTH - seq.shape[0]\n",
    "    seq = np.pad(seq, ((0, pad_width), (0, 0), (0, 0)), mode='constant')\n",
    "            \n",
    "else:\n",
    "    seq = seq[:SEQUENCE_LENGTH]\n",
    "\n",
    "# Aplanar a (30, 126)\n",
    "seq = seq.reshape(SEQUENCE_LENGTH, -1)\n",
    "\n",
    "# ─── NORMALIZAR ───────────────────────────────────────────────────────────────\n",
    "with open(scaler_path, \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n",
    "seq = (seq - scaler['mean']) / scaler['std']\n",
    "tensor = torch.from_numpy(seq.astype(np.float32)).unsqueeze(0)\n",
    "\n",
    "# ─── CARGAR MODELO ────────────────────────────────────────────────────────────\n",
    "model = SignLSTM(input_size=126, hidden_size=128, num_layers=2, num_classes=15)\n",
    "model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "# ─── INFERENCIA ───────────────────────────────────────────────────────────────\n",
    "with torch.no_grad():\n",
    "    logits = model(tensor)\n",
    "    probs = torch.softmax(logits, dim=1).numpy()[0]\n",
    "    predicted_idx = int(probs.argmax())\n",
    "\n",
    "# ─── DECODIFICAR PREDICCIÓN ───────────────────────────────────────────────────\n",
    "with open(label_encoder_path, \"rb\") as f:\n",
    "    le = pickle.load(f)\n",
    "\n",
    "print(f\"\\n🔍 Prediction for file: {npy_files[100]}\")\n",
    "print(f\"👉 Predicted sign: {le.inverse_transform([predicted_idx])[0]}\")\n",
    "print(\"📊 Probabilities:\")\n",
    "for i, p in enumerate(probs):\n",
    "    print(f\"  {le.inverse_transform([i])[0]}: {p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb869caa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
